{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This Task firstly transfer the origin data set into the extracted csv files. \n",
    "2. Format the csv files of seprators and blank values.\n",
    "3. Get connection with the GBQ,to create and upload the datas\n",
    "\n",
    "**Notesï¼šI have finished the steps needed in the task1, and did some tests on some files. It all worked well. But due to the network reason,I can't upload all of these files into the GBQ, so I copied the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBQ Setting\n",
    "service_path = \"./\"\n",
    "service_file = 'Hong-Wedge-8a5b036bb32c.json' \n",
    "gbq_proj_id = 'hong-wedge' \n",
    "\n",
    "private_key =service_path + service_file\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "input_dir=\"WedgeZipFiles/ZipFiles/\"\n",
    "output_dir=\"WedgeZipFiles/DataFiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(table_name):\n",
    "    table_id = \"hong-wedge.transactions.\"+table_name\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"datetime\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"register_no\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"emp_no\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"trans_no\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"upc\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"description\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"trans_type\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"trans_subtype\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"trans_status\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"department\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"quantity\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"scale\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"cost\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"unitPrice\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"total\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"regPrice\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"altPrice\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"tax\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"taxexempt\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"foodstamp\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"wicable\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"discount\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"memDiscount\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"discountable\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"discounttype\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"voided\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"percentDiscount\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"ItemQtty\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"volDiscType\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"volume\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"VolSpecial\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"mixMatch\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"matched\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"memType\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"staff\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"numflag\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"itemstatus\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"tenderstatus\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"charflag\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"varflag\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"local\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"organic\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"display\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"receipt\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"card_no\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"store\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"branch\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"match_id\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"trans_id\", \"FLOAT\"),\n",
    "    ]\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(table_name,data_file):\n",
    "    \n",
    "    table_id = \"hong-wedge.transactions.\"+table_name\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV, skip_leading_rows=0, autodetect=False,)\n",
    "    \n",
    "    with open(output_dir+data_file, \"rb\") as source_file:\n",
    "        print(\"Start to upload data to table {}\",table_id)\n",
    "        job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "        job.result()  # Waits for the job to complete.\n",
    "        table = client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(table_name,datas):\n",
    "    table_id = \"hong-wedge.transactions.\"+table_name\n",
    "    rows_to_insert = [\n",
    "    {\n",
    "        u\"datetime\": data[0], \n",
    "        u\"register_no\": data[1],\n",
    "        u\"emp_no\": data[2],\n",
    "        u\"trans_no\": data[3], \n",
    "        u\"upc\": data[4],\n",
    "        u\"description\": data[5],\n",
    "        u\"trans_type\": data[6], \n",
    "        u\"trans_subtype\": data[7],\n",
    "        u\"trans_status\": data[8],\n",
    "        u\"department\": data[9], \n",
    "        u\"quantity\": data[10],\n",
    "        u\"scale\": data[11],\n",
    "        u\"cost\": data[12], \n",
    "        u\"unitPrice\": data[13],\n",
    "        u\"total\": data[14],\n",
    "        u\"regPrice\": data[15], \n",
    "        u\"altPrice\": data[16],\n",
    "        u\"tax\": data[17],\n",
    "        u\"taxexempt\": data[18], \n",
    "        u\"foodstamp\": data[19],\n",
    "        u\"wicable\": data[20],\n",
    "        u\"discount\": data[21], \n",
    "        u\"memDiscount\": data[22],\n",
    "        u\"discountable\": data[23],\n",
    "        u\"discounttype\": data[24], \n",
    "        u\"voided\": data[25], \n",
    "        u\"percentDiscount\": data[26],\n",
    "        u\"ItemQtty\": data[27],\n",
    "        u\"volDiscType\": data[28], \n",
    "        u\"volume\": data[29],\n",
    "        u\"VolSpecial\": data[30],\n",
    "        u\"mixMatch\": data[31], \n",
    "        u\"matched\": data[32],\n",
    "        u\"memType\": data[33],\n",
    "        u\"staff\": data[34], \n",
    "        u\"numflag\": data[35],\n",
    "        u\"itemstatus\": data[36],\n",
    "        u\"tenderstatus\": data[37], \n",
    "        u\"charflag\": data[38],\n",
    "        u\"varflag\": data[39],\n",
    "        u\"batchHeaderID\": data[40], \n",
    "        u\"local\": data[41],\n",
    "        u\"organic\": data[42],\n",
    "        u\"display\": data[43], \n",
    "        u\"receipt\": data[44],\n",
    "        u\"card_no\": data[45],\n",
    "        u\"store\": data[46], \n",
    "        u\"branch\": data[47],\n",
    "        u\"branch\": data[48],\n",
    "        u\"trans_id\": data[49], \n",
    "    }]\n",
    "        \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert, row_ids=[None] * len(rows_to_insert))\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(\"Encountered errors while inserting rows: {}\".format(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(file_name):\n",
    "    input_file = zf.open(file_name,'r')\n",
    "    input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "    dialect = csv.Sniffer().sniff(sample=input_file.readline(),         # Data Format: Multi Delimiters\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "    table_name=file_name.split(\".\")[0]\n",
    "    output_file_name=table_name+\"_clean.csv\"\n",
    "    f = open(output_dir+output_file_name,'w',encoding=\"utf-8\")\n",
    "\n",
    "    for idx,line in enumerate(input_file):\n",
    "        data=line.replace(\"\\\\N\",\"\").replace(\"NULL\",\"\").replace(\"None\",\"\").strip().split(dialect.delimiter)\n",
    "        data=\",\".join(t.strip('\"\"') for t in data)\n",
    "        data+=\"\\n\"\n",
    "        f.write(data)\n",
    "    input_file.close() \n",
    "    f.close()\n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read >>> transArchive_201001_201003.csv\n",
      "Clean >>> transArchive_201001_201003.csv\n",
      "Create >>> transArchive_201001_201003.csv\n",
      "Upload >>> transArchive_201001_201003.csv\n",
      "Done 1/53>>>transArchive_201001_201003.csv \n",
      "Read >>> transArchive_201004_201006.csv\n",
      "Clean >>> transArchive_201004_201006.csv\n",
      "Create >>> transArchive_201004_201006.csv\n",
      "Upload >>> transArchive_201004_201006.csv\n",
      "Done 2/53>>>transArchive_201004_201006.csv \n",
      "Read >>> transArchive_201007_201009.csv\n",
      "Clean >>> transArchive_201007_201009.csv\n",
      "Create >>> transArchive_201007_201009.csv\n",
      "Upload >>> transArchive_201007_201009.csv\n",
      "Done 3/53>>>transArchive_201007_201009.csv \n",
      "Read >>> transArchive_201010_201012.csv\n",
      "Clean >>> transArchive_201010_201012.csv\n",
      "Create >>> transArchive_201010_201012.csv\n",
      "Upload >>> transArchive_201010_201012.csv\n",
      "Done 4/53>>>transArchive_201010_201012.csv \n",
      "Read >>> transArchive_201101_201103.csv\n",
      "Clean >>> transArchive_201101_201103.csv\n",
      "Create >>> transArchive_201101_201103.csv\n",
      "Upload >>> transArchive_201101_201103.csv\n",
      "Done 5/53>>>transArchive_201101_201103.csv \n",
      "Read >>> transArchive_201104.csv\n",
      "Clean >>> transArchive_201104.csv\n",
      "Create >>> transArchive_201104.csv\n",
      "Upload >>> transArchive_201104.csv\n",
      "Done 6/53>>>transArchive_201104.csv \n",
      "Read >>> transArchive_201105.csv\n",
      "Clean >>> transArchive_201105.csv\n",
      "Create >>> transArchive_201105.csv\n",
      "Upload >>> transArchive_201105.csv\n",
      "Done 7/53>>>transArchive_201105.csv \n",
      "Read >>> transArchive_201106.csv\n",
      "Clean >>> transArchive_201106.csv\n",
      "Create >>> transArchive_201106.csv\n",
      "Upload >>> transArchive_201106.csv\n",
      "Done 8/53>>>transArchive_201106.csv \n",
      "Read >>> transArchive_201107_201109.csv\n",
      "Clean >>> transArchive_201107_201109.csv\n",
      "Create >>> transArchive_201107_201109.csv\n",
      "Upload >>> transArchive_201107_201109.csv\n",
      "Done 9/53>>>transArchive_201107_201109.csv \n",
      "Read >>> transArchive_201110_201112.csv\n",
      "Clean >>> transArchive_201110_201112.csv\n",
      "Create >>> transArchive_201110_201112.csv\n",
      "Upload >>> transArchive_201110_201112.csv\n",
      "Done 10/53>>>transArchive_201110_201112.csv \n",
      "Read >>> transArchive_201201_201203.csv\n",
      "Clean >>> transArchive_201201_201203.csv\n",
      "Create >>> transArchive_201201_201203.csv\n",
      "Upload >>> transArchive_201201_201203.csv\n",
      "Done 11/53>>>transArchive_201201_201203.csv \n",
      "Read >>> transArchive_201201_201203_inactive.csv\n",
      "Clean >>> transArchive_201201_201203_inactive.csv\n",
      "Create >>> transArchive_201201_201203_inactive.csv\n",
      "Upload >>> transArchive_201201_201203_inactive.csv\n",
      "Done 12/53>>>transArchive_201201_201203_inactive.csv \n",
      "Read >>> transArchive_201204_201206.csv\n",
      "Clean >>> transArchive_201204_201206.csv\n",
      "Create >>> transArchive_201204_201206.csv\n",
      "Upload >>> transArchive_201204_201206.csv\n",
      "Done 13/53>>>transArchive_201204_201206.csv \n",
      "Read >>> transArchive_201204_201206_inactive.csv\n",
      "Clean >>> transArchive_201204_201206_inactive.csv\n",
      "Create >>> transArchive_201204_201206_inactive.csv\n",
      "Upload >>> transArchive_201204_201206_inactive.csv\n",
      "Done 14/53>>>transArchive_201204_201206_inactive.csv \n",
      "Read >>> transArchive_201207_201209.csv\n",
      "Clean >>> transArchive_201207_201209.csv\n",
      "Create >>> transArchive_201207_201209.csv\n",
      "Upload >>> transArchive_201207_201209.csv\n",
      "Done 15/53>>>transArchive_201207_201209.csv \n",
      "Read >>> transArchive_201207_201209_inactive.csv\n",
      "Clean >>> transArchive_201207_201209_inactive.csv\n",
      "Create >>> transArchive_201207_201209_inactive.csv\n",
      "Upload >>> transArchive_201207_201209_inactive.csv\n",
      "Done 16/53>>>transArchive_201207_201209_inactive.csv \n",
      "Read >>> transArchive_201210_201212.csv\n",
      "Clean >>> transArchive_201210_201212.csv\n",
      "Create >>> transArchive_201210_201212.csv\n",
      "Upload >>> transArchive_201210_201212.csv\n",
      "Done 17/53>>>transArchive_201210_201212.csv \n",
      "Read >>> transArchive_201210_201212_inactive.csv\n",
      "Clean >>> transArchive_201210_201212_inactive.csv\n",
      "Create >>> transArchive_201210_201212_inactive.csv\n",
      "Upload >>> transArchive_201210_201212_inactive.csv\n",
      "Done 18/53>>>transArchive_201210_201212_inactive.csv \n",
      "Read >>> transArchive_201301_201303.csv\n",
      "Clean >>> transArchive_201301_201303.csv\n",
      "Create >>> transArchive_201301_201303.csv\n",
      "Upload >>> transArchive_201301_201303.csv\n",
      "Done 19/53>>>transArchive_201301_201303.csv \n",
      "Read >>> transArchive_201301_201303_inactive.csv\n",
      "Clean >>> transArchive_201301_201303_inactive.csv\n",
      "Create >>> transArchive_201301_201303_inactive.csv\n",
      "Upload >>> transArchive_201301_201303_inactive.csv\n",
      "Done 20/53>>>transArchive_201301_201303_inactive.csv \n",
      "Read >>> transArchive_201304_201306.csv\n",
      "Clean >>> transArchive_201304_201306.csv\n",
      "Create >>> transArchive_201304_201306.csv\n",
      "Upload >>> transArchive_201304_201306.csv\n",
      "Done 21/53>>>transArchive_201304_201306.csv \n",
      "Read >>> transArchive_201304_201306_inactive.csv\n",
      "Clean >>> transArchive_201304_201306_inactive.csv\n",
      "Create >>> transArchive_201304_201306_inactive.csv\n",
      "Upload >>> transArchive_201304_201306_inactive.csv\n",
      "Done 22/53>>>transArchive_201304_201306_inactive.csv \n",
      "Read >>> transArchive_201307_201309.csv\n",
      "Clean >>> transArchive_201307_201309.csv\n",
      "Create >>> transArchive_201307_201309.csv\n",
      "Upload >>> transArchive_201307_201309.csv\n",
      "Done 23/53>>>transArchive_201307_201309.csv \n",
      "Read >>> transArchive_201307_201309_inactive.csv\n",
      "Clean >>> transArchive_201307_201309_inactive.csv\n",
      "Create >>> transArchive_201307_201309_inactive.csv\n",
      "Upload >>> transArchive_201307_201309_inactive.csv\n",
      "Done 24/53>>>transArchive_201307_201309_inactive.csv \n",
      "Read >>> transArchive_201310_201312.csv\n",
      "Clean >>> transArchive_201310_201312.csv\n",
      "Create >>> transArchive_201310_201312.csv\n",
      "Upload >>> transArchive_201310_201312.csv\n",
      "Done 25/53>>>transArchive_201310_201312.csv \n",
      "Read >>> transArchive_201310_201312_inactive.csv\n",
      "Clean >>> transArchive_201310_201312_inactive.csv\n",
      "Create >>> transArchive_201310_201312_inactive.csv\n",
      "Upload >>> transArchive_201310_201312_inactive.csv\n",
      "Done 26/53>>>transArchive_201310_201312_inactive.csv \n",
      "Read >>> transArchive_201401_201403.csv\n",
      "Clean >>> transArchive_201401_201403.csv\n",
      "Create >>> transArchive_201401_201403.csv\n",
      "Upload >>> transArchive_201401_201403.csv\n",
      "Done 27/53>>>transArchive_201401_201403.csv \n",
      "Read >>> transArchive_201401_201403_inactive.csv\n",
      "Clean >>> transArchive_201401_201403_inactive.csv\n",
      "Create >>> transArchive_201401_201403_inactive.csv\n",
      "Upload >>> transArchive_201401_201403_inactive.csv\n",
      "Done 28/53>>>transArchive_201401_201403_inactive.csv \n",
      "Read >>> transArchive_201404_201406.csv\n",
      "Clean >>> transArchive_201404_201406.csv\n",
      "Create >>> transArchive_201404_201406.csv\n",
      "Upload >>> transArchive_201404_201406.csv\n",
      "Done 29/53>>>transArchive_201404_201406.csv \n",
      "Read >>> transArchive_201404_201406_inactive.csv\n",
      "Clean >>> transArchive_201404_201406_inactive.csv\n",
      "Create >>> transArchive_201404_201406_inactive.csv\n",
      "Upload >>> transArchive_201404_201406_inactive.csv\n",
      "Done 30/53>>>transArchive_201404_201406_inactive.csv \n",
      "Read >>> transArchive_201407_201409.csv\n",
      "Clean >>> transArchive_201407_201409.csv\n",
      "Create >>> transArchive_201407_201409.csv\n",
      "Upload >>> transArchive_201407_201409.csv\n",
      "Done 31/53>>>transArchive_201407_201409.csv \n",
      "Read >>> transArchive_201407_201409_inactive.csv\n",
      "Clean >>> transArchive_201407_201409_inactive.csv\n",
      "Create >>> transArchive_201407_201409_inactive.csv\n",
      "Upload >>> transArchive_201407_201409_inactive.csv\n",
      "Done 32/53>>>transArchive_201407_201409_inactive.csv \n",
      "Read >>> transArchive_201410_201412.csv\n",
      "Clean >>> transArchive_201410_201412.csv\n",
      "Create >>> transArchive_201410_201412.csv\n",
      "Upload >>> transArchive_201410_201412.csv\n",
      "Done 33/53>>>transArchive_201410_201412.csv \n",
      "Read >>> transArchive_201410_201412_inactive.csv\n",
      "Clean >>> transArchive_201410_201412_inactive.csv\n",
      "Create >>> transArchive_201410_201412_inactive.csv\n",
      "Upload >>> transArchive_201410_201412_inactive.csv\n",
      "Done 34/53>>>transArchive_201410_201412_inactive.csv \n",
      "Read >>> transArchive_201501_201503.csv\n",
      "Clean >>> transArchive_201501_201503.csv\n",
      "Create >>> transArchive_201501_201503.csv\n",
      "Upload >>> transArchive_201501_201503.csv\n",
      "Done 35/53>>>transArchive_201501_201503.csv \n",
      "Read >>> transArchive_201504_201506.csv\n",
      "Clean >>> transArchive_201504_201506.csv\n",
      "Create >>> transArchive_201504_201506.csv\n",
      "Upload >>> transArchive_201504_201506.csv\n",
      "Done 36/53>>>transArchive_201504_201506.csv \n",
      "Read >>> transArchive_201507_201509.csv\n",
      "Clean >>> transArchive_201507_201509.csv\n",
      "Create >>> transArchive_201507_201509.csv\n",
      "Upload >>> transArchive_201507_201509.csv\n",
      "Done 37/53>>>transArchive_201507_201509.csv \n",
      "Read >>> transArchive_201510.csv\n",
      "Clean >>> transArchive_201510.csv\n",
      "Create >>> transArchive_201510.csv\n",
      "Upload >>> transArchive_201510.csv\n",
      "Done 38/53>>>transArchive_201510.csv \n",
      "Read >>> transArchive_201511.csv\n",
      "Clean >>> transArchive_201511.csv\n",
      "Create >>> transArchive_201511.csv\n",
      "Upload >>> transArchive_201511.csv\n",
      "Done 39/53>>>transArchive_201511.csv \n",
      "Read >>> transArchive_201512.csv\n",
      "Clean >>> transArchive_201512.csv\n",
      "Create >>> transArchive_201512.csv\n",
      "Upload >>> transArchive_201512.csv\n",
      "Done 40/53>>>transArchive_201512.csv \n",
      "Read >>> transArchive_201601.csv\n",
      "Clean >>> transArchive_201601.csv\n",
      "Create >>> transArchive_201601.csv\n",
      "Upload >>> transArchive_201601.csv\n",
      "Done 41/53>>>transArchive_201601.csv \n",
      "Read >>> transArchive_201602.csv\n",
      "Clean >>> transArchive_201602.csv\n",
      "Create >>> transArchive_201602.csv\n",
      "Upload >>> transArchive_201602.csv\n",
      "Done 42/53>>>transArchive_201602.csv \n",
      "Read >>> transArchive_201603.csv\n",
      "Clean >>> transArchive_201603.csv\n",
      "Create >>> transArchive_201603.csv\n",
      "Upload >>> transArchive_201603.csv\n",
      "Done 43/53>>>transArchive_201603.csv \n",
      "Read >>> transArchive_201604.csv\n",
      "Clean >>> transArchive_201604.csv\n",
      "Create >>> transArchive_201604.csv\n",
      "Upload >>> transArchive_201604.csv\n",
      "Done 44/53>>>transArchive_201604.csv \n",
      "Read >>> transArchive_201605.csv\n",
      "Clean >>> transArchive_201605.csv\n",
      "Create >>> transArchive_201605.csv\n",
      "Upload >>> transArchive_201605.csv\n",
      "Done 45/53>>>transArchive_201605.csv \n",
      "Read >>> transArchive_201606.csv\n",
      "Clean >>> transArchive_201606.csv\n",
      "Create >>> transArchive_201606.csv\n",
      "Upload >>> transArchive_201606.csv\n",
      "Done 46/53>>>transArchive_201606.csv \n",
      "Read >>> transArchive_201607.csv\n",
      "Clean >>> transArchive_201607.csv\n",
      "Create >>> transArchive_201607.csv\n",
      "Upload >>> transArchive_201607.csv\n",
      "Done 47/53>>>transArchive_201607.csv \n",
      "Read >>> transArchive_201608.csv\n",
      "Clean >>> transArchive_201608.csv\n",
      "Create >>> transArchive_201608.csv\n",
      "Upload >>> transArchive_201608.csv\n",
      "Done 48/53>>>transArchive_201608.csv \n",
      "Read >>> transArchive_201609.csv\n",
      "Clean >>> transArchive_201609.csv\n",
      "Create >>> transArchive_201609.csv\n",
      "Upload >>> transArchive_201609.csv\n",
      "Done 49/53>>>transArchive_201609.csv \n",
      "Read >>> transArchive_201610.csv\n",
      "Clean >>> transArchive_201610.csv\n",
      "Create >>> transArchive_201610.csv\n",
      "Upload >>> transArchive_201610.csv\n",
      "Done 50/53>>>transArchive_201610.csv \n",
      "Read >>> transArchive_201611.csv\n",
      "Clean >>> transArchive_201611.csv\n",
      "Create >>> transArchive_201611.csv\n",
      "Upload >>> transArchive_201611.csv\n",
      "Done 51/53>>>transArchive_201611.csv \n",
      "Read >>> transArchive_201612.csv\n",
      "Clean >>> transArchive_201612.csv\n",
      "Create >>> transArchive_201612.csv\n",
      "Upload >>> transArchive_201612.csv\n",
      "Done 52/53>>>transArchive_201612.csv \n",
      "Read >>> transArchive_201701.csv\n",
      "Clean >>> transArchive_201701.csv\n",
      "Create >>> transArchive_201701.csv\n",
      "Upload >>> transArchive_201701.csv\n",
      "Done 53/53>>>transArchive_201701.csv \n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "zip_files=os.listdir(input_dir)\n",
    "count=1\n",
    "total=53\n",
    "for zip_file in zip_files:\n",
    "    with ZipFile(input_dir+ zip_file, 'r') as zf : \n",
    "        for name in zf.namelist():\n",
    "            print(\"Read >>> \"+name)\n",
    "            print(\"Clean >>> \"+name)\n",
    "            output_file=clean_file(name)\n",
    "            print(\"Create >>> \"+name)\n",
    "            table_name=name.split(\".\")[0]\n",
    "            create_table(table_name)\n",
    "            print(\"Upload >>> \"+name)\n",
    "            upload_data(table_name,output_file)\n",
    "            print(\"Done {}/{}>>>{} \".format(str(count),str(total),name))\n",
    "            count=count+1\n",
    "print('All Done!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data generated in Task2 for use in Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read >>> sample_owners_records.csv\n",
      "Clean >>> sample_owners_records.csv\n",
      "Create >>> sample_owners_records.csv\n",
      "Upload >>> sample_owners_records.csv\n",
      "Start to upload data to table {} hong-wedge.transactions.sample_owners_records\n",
      "Loaded 992200 rows and 50 columns to hong-wedge.transactions.sample_owners_records\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "zip_file=\"WedgeZipFiles/sample_owners_records.zip\"\n",
    "with ZipFile(zip_file, 'r') as zf : \n",
    "    for name in zf.namelist():\n",
    "        print(\"Read >>> \"+name)\n",
    "        print(\"Clean >>> \"+name)\n",
    "        output_file=clean_file(name)\n",
    "        print(\"Create >>> \"+name)\n",
    "        table_name=name.split(\".\")[0]\n",
    "#         create_table(table_name)\n",
    "        print(\"Upload >>> \"+name)\n",
    "        upload_data(table_name,output_file)\n",
    "    print('All Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
